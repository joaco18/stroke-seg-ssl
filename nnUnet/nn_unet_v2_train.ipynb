{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E8tg6g28K9_2"
      },
      "source": [
        "## Structuring the task directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vElW9W57gufC"
      },
      "source": [
        "In this section, we will copy and rename our images according to the structure required by nn-UNet. Both train and validation images will be transfered to imagesTr, the corresponding labels to labelsTr, whereas the test images will be stored in imagesTs.\n",
        "\n",
        "<!-- - Task501 - Old preproc NCCT APIS\n",
        "- Task502 - Old preproc ADC APIS\n",
        "- Task503 - NCCT APIS\n",
        "- Task504 - ADC APIS\n",
        "- Task505 - NCCT AISD\n",
        "- Task506 - DWI AISD -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xm1oO5N0LGQB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import subprocess\n",
        "import yaml\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path().resolve().parent\n",
        "import sys ; sys.path.insert(0, str(repo_root))\n",
        "from utils.utils import get_datasets, hardcore_fix_images\n",
        "\n",
        "# MACRO AND ENV VARIABLES\n",
        "datapath = Path('<DATA_PATH>')\n",
        "base_path = Path('PATH_TO_PROJECT/nnunetv2')\n",
        "os.environ['nnUNet_raw'] = str(base_path/'nnUNet_raw')\n",
        "os.environ['nnUNet_preprocessed'] = str(base_path/'preprocessed')\n",
        "os.environ['nnUNet_results'] = str(base_path/'nnUNet_trained_models')\n",
        "\n",
        "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
        "from nnunetv2.paths import nnUNet_raw, nnUNet_preprocessed\n",
        "from batchgenerators.utilities.file_and_folder_operations import join, save_json, load_json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGS\n",
        "task_id = '056'\n",
        "ref_ssl_task_id = '036'\n",
        "foldername = f'Dataset{task_id}_AIS'\n",
        "ref_ssl_foldername = f'Dataset{ref_ssl_task_id}_AIS'\n",
        "\n",
        "ssl = False\n",
        "dataset_args = {\n",
        "    'datapath': datapath,\n",
        "    'datasets': ['tum', 'apis', 'aisd', 'tbi'] if ssl else ['tum'],\n",
        "    'standard': ['gold', 'silver', '-'],\n",
        "    'pathology': ['ais', 'normal'] if ssl else ['ais'],\n",
        "    'ssl': ssl,\n",
        "    'fold': 0\n",
        "}\n",
        "crop = False\n",
        "use_diff = False\n",
        "use_contralateral = False\n",
        "use_bm_as_msk = False\n",
        "cases_to_exclude = []\n",
        "ssl_pretrained = True\n",
        "\n",
        "# cfg file\n",
        "num_epochs = 100\n",
        "unfreeze_epoch = None\n",
        "unfreeze_lr = None\n",
        "save_at_epochs = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "\n",
        "# Folder generation\n",
        "preproc_gt_path = join(nnUNet_preprocessed, foldername, 'gt_segmentations')\n",
        "out_base = join(nnUNet_raw, foldername)\n",
        "imagestr, labelstr = join(out_base, \"imagesTr\"), join(out_base, \"labelsTr\")\n",
        "imagests, labelsts = join(out_base, 'imagesTs'), join(out_base, \"labelsTs\")\n",
        "for i in [imagestr, labelstr, imagests, labelsts]:\n",
        "    Path(i).mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mY7iXCKkOp2J"
      },
      "source": [
        "## Copying the datas to the specific dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aQRoK9FPh5HJ"
      },
      "source": [
        "In this section we will copy the raw ISBR data to the corresponding directories as previously explained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying files...\n",
            "\n",
            "Copying train files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 111/111 [00:09<00:00, 11.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying val files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 29/29 [00:02<00:00,  9.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying test files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 19/19 [00:02<00:00,  8.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dataset.json file...\n",
            "\n",
            "Fingerprint extraction...\n",
            "Dataset055_AIS\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "\n",
            "####################\n",
            "verify_dataset_integrity Done. \n",
            "If you didn't see any error messages then your dataset is most likely OK!\n",
            "####################\n",
            "\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:12<00:00, 11.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment planning...\n",
            "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
            "3D fullres U-Net configuration:\n",
            "{'data_identifier': 'nnUNetPlansSSL_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': array([ 96, 160, 160]), 'median_image_size_in_voxels': array([140., 250., 219.]), 'spacing': array([1., 1., 1.]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'num_pool_per_axis': [4, 5, 5], 'pool_op_kernel_sizes': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'unet_max_num_features': 320, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': False}\n",
            "\n",
            "Plans were saved to /home/jseia/Desktop/thesis/code/nnUNet_ais/nnunetv2/preprocessed/Dataset055_AIS/nnUNetPlansSSL.json\n",
            "Preprocessing...\n",
            "Preprocessing dataset Dataset055_AIS\n",
            "Configuration: 3d_fullres...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:21<00:00,  6.58it/s]\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating splits file...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n",
            "WARNING:root:Be aware that since since ncct-tilt the image provided in ncct field will be the ncct-tilt. GT masks will also be adjusted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating dataset fingerprints file...\n",
            "\n",
            "Preprocessing dataset with new fingerprints file...\n",
            "\n",
            "Preprocessing dataset Dataset055_AIS\n",
            "Configuration: 3d_fullres...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:22<00:00,  6.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving configuration files...\n",
            "\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "def copy_fn(i, set_dset, use_diff, set_dir, lab_dir, use_bm_as_msk):\n",
        "    sample = set_dset[i]\n",
        "    sample_id = sample['subject']\n",
        "    dataset_name = sample['dataset_name']\n",
        "\n",
        "    # original filenames\n",
        "    names = sample['crop'] if crop else sample['clean']\n",
        "    # print(sample)\n",
        "    try:\n",
        "        ncct_path = names['ncct-pp']\n",
        "    except:\n",
        "        print(sample_id, sample)\n",
        "        raise Exception()\n",
        "    try:\n",
        "        msk_path = names['bm'] if use_bm_as_msk else names['msk-pp']\n",
        "    except:\n",
        "        print(sample_id, sample)\n",
        "        raise Exception()\n",
        "    # new filenames\n",
        "    new_ncct_name = f'{sample_id}_0000.nii.gz'\n",
        "    new_mask_name = f'{sample_id}.nii.gz'\n",
        "    # copy image to task folder\n",
        "    shutil.copyfile(ncct_path, set_dir/new_ncct_name)\n",
        "    hardcore_fix_images(msk_path, ncct_path, lab_dir/new_mask_name)\n",
        "    # shutil.copyfile(msk_path, lab_dir/new_mask_name)\n",
        "\n",
        "    # if diff image required:\n",
        "    if use_diff:\n",
        "        try:\n",
        "            diff_path = names['diff-pp']\n",
        "        except:\n",
        "            print(sample_id, sample)\n",
        "            raise Exception()\n",
        "        new_diff_name = f'{sample_id}_0001.nii.gz'\n",
        "        hardcore_fix_images(diff_path, ncct_path, set_dir/new_diff_name)\n",
        "        # shutil.copyfile(diff_path, set_dir/new_diff_name)\n",
        "    if use_contralateral:\n",
        "        try:\n",
        "            flip_path = names['ncct-pp-flip']\n",
        "        except:\n",
        "            print(sample_id, sample)\n",
        "            raise Exception()\n",
        "        new_flip_name = f'{sample_id}_0001.nii.gz'\n",
        "        hardcore_fix_images(flip_path, ncct_path, set_dir/new_flip_name)\n",
        "\n",
        "# Get the datasets\n",
        "train, validation, test = get_datasets(**dataset_args)\n",
        "\n",
        "dsets = [train, validation, test]\n",
        "data_dirs = [imagestr, imagestr, imagests]\n",
        "labels_dirs = [labelstr, labelstr, labelsts]\n",
        "parttns = ['train', 'val', 'test']\n",
        "\n",
        "# Copy files\n",
        "print('Copying files...\\n')\n",
        "for set_dset, set_dir, lab_dir, ptn in zip(dsets, data_dirs, labels_dirs, parttns):\n",
        "    print(f'Copying {ptn} files...')\n",
        "    set_dir, lab_dir = Path(set_dir), Path(lab_dir)\n",
        "    parallel_copy = partial(copy_fn, set_dset=set_dset, use_diff=use_diff,\n",
        "                            set_dir=set_dir, lab_dir=lab_dir, use_bm_as_msk=use_bm_as_msk)\n",
        "    with mp.Pool(mp.cpu_count()) as pool:\n",
        "        for _ in tqdm(pool.imap(parallel_copy, range(len(set_dset))),\n",
        "                      total=len(set_dset)):\n",
        "            pass\n",
        "\n",
        "file_endings = ['_0000.nii.gz', '_0001.nii.gz'] if (use_diff or use_contralateral) else ['_0000.nii.gz']\n",
        "\n",
        "# Creating the dataset.json file required by nn-UNet which contains information \n",
        "# about our custom dataset.\n",
        "print('Creating dataset.json file...\\n')\n",
        "train_list = [i for i in train.df.subject.tolist() if i not in cases_to_exclude]\n",
        "val_list = [i for i in validation.df.subject.tolist() if i not in cases_to_exclude]\n",
        "channel_names = {0: 'CT'}\n",
        "if use_diff:\n",
        "    channel_names = {0: 'CT', 1: 'rescale_to_0_1'}\n",
        "if use_contralateral:\n",
        "    channel_names = {0: 'CT', 1: 'CT'}\n",
        "\n",
        "generate_dataset_json(\n",
        "    out_base,\n",
        "    channel_names=channel_names,\n",
        "    labels={'background': 0, 'ais': 1},\n",
        "    num_training_cases=len(val_list+train_list),\n",
        "    file_ending='.nii.gz'\n",
        ")\n",
        "\n",
        "# The last step of this section is to check if the structure of our dataset directory is\n",
        "# compatible with nn-UNet's requirements.\n",
        "command = 'nnUNetv2_plan_and_preprocess -pl ExperimentPlannerSSL -overwrite_plans_name nnUNetPlansSSL ' \\\n",
        "           f'-d {task_id} -np 8 -c 3d_fullres --verify_dataset_integrity'\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "## Creating Train-Val split\n",
        "print('Creating splits file...\\n')\n",
        "raw_splits = []\n",
        "splits = []\n",
        "for i in range(5):\n",
        "    dataset_args['fold'] = i\n",
        "    train, validation, _ = get_datasets(**dataset_args)\n",
        "    train_list = [j for j in train.df.subject.tolist() if i not in cases_to_exclude]\n",
        "    val_list = [j for j in validation.df.subject.tolist() if i not in cases_to_exclude]\n",
        "    raw_split = []\n",
        "    for k, fend in enumerate(file_endings):\n",
        "        raw_split.append([f'{imagestr}/{j}{fend}' for j in val_list])\n",
        "    raw_splits.append(raw_split)\n",
        "    splits.append({'train': train_list, 'val': val_list})\n",
        "save_json(splits, f'{nnUNet_preprocessed}/{foldername}/splits_final.json')\n",
        "save_json(raw_splits, f'{nnUNet_preprocessed}/{foldername}/raw_splits_final.json')\n",
        "\n",
        "\n",
        "# Configuration files paths\n",
        "base_file = Path(f'{nnUNet_preprocessed}/{foldername}/nnunet_cfg_base.yml')\n",
        "run_file = Path(f'{nnUNet_preprocessed}/{foldername}/nnunet_cfg.yml')\n",
        "cfg_dict = {}\n",
        "\n",
        "## Overwrite dataset fingerprint to normalize the images according to SSL values\n",
        "if ssl_pretrained:\n",
        "    print('Updating dataset fingerprints file...\\n')\n",
        "    # Get dataset fingerprint from ssl dataset\n",
        "    ssl_dfing_path = Path(f'{nnUNet_preprocessed}/{ref_ssl_foldername}/dataset_fingerprint.json')\n",
        "    with open(ssl_dfing_path, 'r') as jfile:\n",
        "        ssl_dfing = json.load(jfile)\n",
        "    intensities_key = 'foreground_intensity_properties_per_channel'\n",
        "\n",
        "    # Modify supervised training files\n",
        "    files_to_change = ['dataset_fingerprint.json', 'nnUNetPlansSSL.json']\n",
        "    for file in files_to_change:\n",
        "        supvs_file_path = Path(f'{nnUNet_preprocessed}/{foldername}/{file}')\n",
        "        with open(supvs_file_path, 'r') as jfile:\n",
        "            supvs_file = json.load(jfile)\n",
        "        supvs_file[intensities_key].update(ssl_dfing[intensities_key])\n",
        "        save_json(supvs_file, supvs_file_path)\n",
        "    \n",
        "    # Reprocess dataset\n",
        "    print('Preprocessing dataset with new fingerprints file...\\n')\n",
        "    subprocess.run(f'rm -r {nnUNet_preprocessed}/{foldername}/nnUNetPlansSSL_3d_fullres', shell=True)\n",
        "    subprocess.run(f'rm -r {nnUNet_preprocessed}/{foldername}/gt_segmentations', shell=True)\n",
        "    subprocess.run(f'nnUNetv2_preprocess -plans_name nnUNetPlansSSL -d {task_id} -np 8 -c 3d_fullres', shell=True)\n",
        "    src = f'{nnUNet_preprocessed}/{ref_ssl_foldername}/nnunet_cfg.yml'\n",
        "    subprocess.run(f'cp {src} {run_file}', shell=True)\n",
        "\n",
        "    # Write configuration file\n",
        "    with open(run_file, 'r') as yfile:\n",
        "        cfg_dict = yaml.safe_load(yfile)\n",
        "\n",
        "print('Saving configuration files...\\n')\n",
        "cfg_dict.update({\n",
        "    'num_epochs': num_epochs,\n",
        "    'unfreeze_epoch': unfreeze_epoch,\n",
        "    'unfreeze_lr': unfreeze_lr,\n",
        "    'ssl_pretrained': ssl_pretrained,\n",
        "    'save_at_epochs': save_at_epochs\n",
        "})\n",
        "\n",
        "with open(base_file, 'w') as yfile:\n",
        "    yaml.dump(cfg_dict, yfile)\n",
        "with open(run_file, 'w') as yfile:\n",
        "    yaml.dump(cfg_dict, yfile)\n",
        "\n",
        "print('Finished')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "e6875e310b9c4a1bfe64034d689950a8b967fa8cc25e58662c520a588e185895"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
